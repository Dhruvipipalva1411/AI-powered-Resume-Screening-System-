# -*- coding: utf-8 -*-
"""Resume_screening.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuG7qsIECjrfbb_Ag7k5jvhA9qZruotF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nlp = spacy.load("en_core_web_sm")

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/Project/resume_screening/Resume/Resume.csv')
resume_folder = '/content/drive/MyDrive/Project/resume_screening/data'

import os
from pdfminer.high_level import extract_text as extract_pdf_text
from docx import Document

def parse_pdf(file_path):
    try:
        text = extract_pdf_text(file_path)
        return text
    except Exception as e:
        return f"Error parsing PDF: {e}"


def parse_docx(file_path):
    try:
        doc = Document(file_path)
        text = '\n'.join([para.text for para in doc.paragraphs])
        return text
    except Exception as e:
        return f"Error parsing DOCX: {e}"

# Unified function to handle both types
def extract_text_from_resume(file_path):
    if file_path.lower().endswith('.pdf'):
        return parse_pdf(file_path)
    elif file_path.lower().endswith('.docx'):
        return parse_docx(file_path)
    else:
        return "Unsupported file format. Only PDF and DOCX are supported."

file_path = "/content/drive/MyDrive/Project/resume_screening/data/10176815.pdf"
resume_text = extract_text_from_resume(file_path)
print(resume_text[:1000])  # Preview first 1000 characters



import os

resume_folder = "/content/drive/MyDrive/Project/resume_screening/data/data"
print("Files in resume folder:", os.listdir(resume_folder))

"""Data Cleaning

"""

import re

def clean_text(text):
    text = text.lower()  # Normalize casing
    text = re.sub(r'\n+', '\n', text)  # Remove multiple newlines
    text = re.sub(r'\s+', ' ', text)   # Replace multiple spaces with single
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII chars (like emojis)
    return text.strip()

import re

def extract_info(text):
    info = {}

    # Extract phone number
    phone_match = re.search(r'\+?\d[\d\s\-]{8,}\d', text)
    info['phone'] = phone_match.group() if phone_match else None

    # Extract skills (keyword match)
    skills_list = ['python', 'java', 'c++', 'machine learning', 'deep learning',
                   'nlp', 'sql', 'html', 'css', 'javascript', 'excel', 'pandas', 'numpy','budgeting', 'marketing', 'sales', 'customer service', 'logistics', 'forecasting']
    found_skills = [skill for skill in skills_list if skill in text]
    info['skills'] = found_skills

    return info

import pandas as pd

resume_data = []

parsed_folder = "/content/drive/MyDrive/Project/resume_screening/Parsed_Text "
for file_name in os.listdir(parsed_folder):
    with open(os.path.join(parsed_folder, file_name), 'r', encoding='utf-8') as f:
        raw_text = f.read()

    clean = clean_text(raw_text)
    extracted = extract_info(clean)
    extracted['filename'] = file_name
    resume_data.append(extracted)

# Convert to DataFrame
df = pd.DataFrame(resume_data)
df.to_csv("/content/drive/MyDrive/Project/resume_screening/Resume/Resume.csv", index=False)
print("‚úÖ Structured data saved to structured_resume_data.csv")
df.head(10)

import os
import re
import spacy
from docx import Document
from pdfminer.high_level import extract_text as extract_pdf_text

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Clean + lemmatize job description
def clean_job_description(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)  # remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc if not token.is_stop]
    return ' '.join(lemmas)

# Parse PDF
def parse_pdf(file_path):
    try:
        return extract_pdf_text(file_path)
    except:
        return ""

# Parse DOCX
def parse_docx(file_path):
    try:
        doc = Document(file_path)
        return '\n'.join([para.text for para in doc.paragraphs])
    except:
        return ""

# Parse TXT
def parse_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except:
        return ""

# Detect and parse any format
def extract_job_text(file_path):
    if file_path.endswith('.pdf'):
        return parse_pdf(file_path)
    elif file_path.endswith('.docx'):
        return parse_docx(file_path)
    elif file_path.endswith('.txt'):
        return parse_txt(file_path)
    return ""

# üî• This is the function you were missing
def load_job_descriptions(folder_path):
    job_descriptions = []
    job_titles = []

    for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        raw_text = extract_job_text(file_path)
        clean_text = clean_job_description(raw_text)

        job_descriptions.append(clean_text)
        job_titles.append(file_name.rsplit('.', 1)[0])  # file name without extension

    return job_descriptions, job_titles

job_desc_folder = "/content/drive/MyDrive/Project/resume_screening/data"
job_descriptions, job_titles = load_job_descriptions(job_desc_folder)

# Preview
for title, desc in zip(job_titles, job_descriptions):
    print(f"üìå {title}: {desc[:500]}...\n")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Suppose you have:
# - a list of resume texts: `resume_texts`
# - a list of cleaned job descriptions: `job_descriptions`
# - and their titles: `job_titles`

def tag_resumes_with_titles(resume_texts, job_descriptions, job_titles):
    # Combine all texts
    all_docs = job_descriptions + resume_texts
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(all_docs)

    # Split back
    job_vecs = tfidf_matrix[:len(job_descriptions)]
    resume_vecs = tfidf_matrix[len(job_descriptions):]

    tags = []
    for i, resume_vec in enumerate(resume_vecs):
        sims = cosine_similarity(resume_vec, job_vecs)
        best_match_idx = sims.argmax()
        tags.append(job_titles[best_match_idx])
    return tags

import sqlite3
import pandas as pd

conn = sqlite3.connect('/content/drive/MyDrive/Project/resume_screening/Resume/resumes.db')
cursor = conn.cursor()

# Create table
cursor.execute("""
CREATE TABLE IF NOT EXISTS resumes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT,
    email TEXT,
    phone TEXT,
    skills TEXT,
    experience TEXT,
    city TEXT,
    filename TEXT
)
""")
conn.commit()

cursor.execute("""
INSERT INTO resumes (name, email, phone, skills, experience, city, filename)
VALUES (?, ?, ?, ?, ?, ?, ?)
""", (
    "John Doe", "john@example.com", "+91 9999999999",
    "python, sql, pandas", "5 years", "Surat", "resume1.txt"
))
conn.commit()
print("‚úÖ Sample data inserted")

results = cursor.execute("SELECT * FROM resumes").fetchall()
for row in results:
    print(row)

conn.close()
print("‚úÖ Connection closed")

"""NLP Modelling"""

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

# Function to tokenize & lemmatize text
def tokenize_and_lemmatize(text):
    doc = nlp(text)
    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]
    return tokens

text = "accountant summary senior level finance management professional highly knowledgeable dedicated ethical senior level management professional credit 20 year expertise facet account financial management financial analysis controllership operate capital budget high profile corporation experience aspect extensive change management corporate environment outsource transfer duty streamlining process provide cost saving solution optimal performance exceptional leadership solution management pertain"

tokens = tokenize_and_lemmatize(text)
print("‚úÖ Tokens & Lemmas:", tokens)

tokenized_texts = [
    ['data', 'scientist', 'analyze', 'dataset'],
    ['chef', 'cook', 'menu', 'design'],
    ...
]

from gensim.models import Word2Vec

# Sample resume texts (you can replace these with real resume text)
resume_texts = [
    "Experienced Python developer with knowledge of Django and APIs.",
    "Executive chef skilled in menu design and food safety.",
    "Data scientist with experience in machine learning and NLP."
]

# Tokenize + Lemmatize each resume
def tokenize(text):
    doc = nlp(text.lower())
    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]

tokenized_texts = [tokenize(text) for text in resume_texts]

# Now train Word2Vec model
w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)

# Save model
w2v_model.save("/content/drive/MyDrive/Project/resume_screening/resume_word2vec.model")
print("‚úÖ Word2Vec model trained and saved.")

vector = w2v_model.wv['python']
print(vector[:10])  # show first 10 dimensions

import numpy as np

def document_vector(tokens, model):
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

# Get vector of a word
print(w2v_model.wv['python'])

# Check similar words
print(w2v_model.wv.most_similar('developer'))

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

df = pd.DataFrame({
    'filename': ['resume1.txt', 'resume2.txt'],
    'raw_text': [
        "Experienced Python developer skilled in APIs, Django and Flask.",
        "Executive Chef with expertise in menu planning, food safety, and kitchen operations."
    ]
})

import re

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)  # remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # remove extra spaces
    return text

df['clean_text'] = df['raw_text'].apply(clean_text)

vectorizer = TfidfVectorizer()

tfidf_matrix = vectorizer.fit_transform(df['clean_text'])

tfidf_array = tfidf_matrix.toarray()

feature_names = vectorizer.get_feature_names_out()

tfidf_df = pd.DataFrame(tfidf_array, columns=feature_names)
print(tfidf_df.head())

df['tfidf_vector'] = list(tfidf_array)

"""Similarity score

"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

resumes = [
    "Experienced Python developer skilled in Django and Flask APIs.",
    "Executive Chef with menu planning and food safety expertise.",
    "Frontend developer with React, HTML, CSS, and UI/UX knowledge."
]

job_descriptions = ["billing accountant summary detail orient billing professional 11 year experience apply exceptional customer service resolve complex billing issue highlight self starter problem resolution deadline orient microsoft office accomplishment research √¢ spreadsheet development employee training development investigate analyze client complaint identify resolve issue multitaske √¢ demonstrate proficiency telephone e mail fax desk reception high volume environment customer service √¢ handle customer effecti"
]

# Combine all texts
all_texts = job_descriptions + resumes

# Vectorize all at once
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(all_texts)

job_vecs = tfidf_matrix[:len(job_descriptions)]
resume_vecs = tfidf_matrix[len(job_descriptions):]

print("‚úÖ Resume Vocabulary:", resumes)
print("‚úÖ Job Description Vocabulary:", job_descriptions)

vectorizer = TfidfVectorizer(
    stop_words='english',
    lowercase=True,
    min_df=1,
    max_df=0.9,
    ngram_range=(1, 2)  # Use unigrams + bigrams
)

vectorizer.fit_transform(resumes + job_descriptions)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Raw inputs
resumes = [
    "Python developer with experience in Flask and Django.",
    "Experienced chef with food safety and menu planning skills.",
    "Frontend React developer familiar with HTML, CSS, and UI/UX."
]

job_descriptions = [
    "Looking for a backend Python developer skilled in Flask.",
    "We need an executive chef to manage food safety and kitchen operations."
]

# Vectorize
vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')
tfidf_matrix = vectorizer.fit_transform(job_descriptions + resumes)

# Split vectors
job_vecs = tfidf_matrix[:len(job_descriptions)]
resume_vecs = tfidf_matrix[len(job_descriptions):]

# Similarity
similarity_matrix = cosine_similarity(resume_vecs, job_vecs)

# Display
pd.DataFrame(similarity_matrix,
             index=[f"Resume {i+1}" for i in range(len(resumes))],
             columns=[f"Job {j+1}" for j in range(len(job_descriptions))])

"""Train model"""

import pandas as pd

# Sample dataset
data = {
    'resume_text': [
        "12065211: senior accountant professional summary senior accountant complete accounting activity accuracy speed extensive experience life cycle general ledger accounting skill aderant cms excel quickbook pro sql access√¢ peachtree hyperion financial report gaap principle ifrs bookkeeping budget development individual tax return essbase work history senior accountant mar 2006 current company city state reconcile balance sheet account limit cash liability account include bank statement purpose maintain accura...",

"12202337: investment accountant career focus accomplish result orient investment professional strong leadership interpersonal skill add energy value organization s quest excellence summary skill internet microsoft office ms word ms power point ms excel pivot table spreadsheet macros business object lombardi eagle accounting system pega dra workbench account reconciliations detail orient accomplishment variance analysis detail orient analytical expert ms office suite account reconciliation expert effective...",

"11759079: senior accountant experience company june 2011 current senior accountant city state prepare quarterly annual financial statement 17 multi family community distribution investor financial institution reconcile account activity income statement balance sheet include cash fix asset derivative equity property debt coordinate review work external audit firm initiate capital call distribution investor manage budget construction loan activity approximately 100 000 000 active construction project fl ma ...",
"11163645: accountant professional summary obtain position fast pace business office environment demand strong organizational technical interpersonal position utilize skill attribute attribute self motivated honest good work ethic effective working cooperative team member reliable hard work thorough completing project commit excellent customer service core qualification intermediate word advance excel powerpoint intermediate access account receivable account payable quickbook enterprise outlook customer se...",
    ], # Removed the extra entry here
    'job_title': [
        "Python Developer",
        "Executive Chef",
        "Python Developer",
        "Data Scientist"
    ],
    'label': [1, 1, 0, 1]  # 1 = relevant, 0 = not relevant
}

df = pd.DataFrame(data)

df['input_text'] = df['resume_text'] + ' [SEP] ' + df['job_title']

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')
X = vectorizer.fit_transform(df['input_text'])
y = df['label']

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Predict
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

# New resumes to rank
new_resumes = [
    "Backend developer with Django, Python, and REST APIs",
    "Executive chef with international menu design experience"
]

job_title = "Python Developer"
new_texts = [r + " [SEP] " + job_title for r in new_resumes]

# Vectorize and predict probabilities
X_new = vectorizer.transform(new_texts)
probs = clf.predict_proba(X_new)[:,1]  # probability of relevance (label = 1)

# Rank by score
ranked = sorted(zip(new_resumes, probs), key=lambda x: x[1], reverse=True)
for i, (resume, score) in enumerate(ranked, 1):
    print(f"Rank {i}: Score={score:.2f} ‚Üí {resume}")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
import pandas as pd

data = {
    'resume_text': [
        "Python backend developer with Flask and Django.",
        "Executive Chef skilled in food safety and kitchen ops.",
        "Frontend developer with HTML, CSS, React, UI/UX.",
        "Senior Data Scientist experienced in ML and NLP.",
        "Chef de cuisine with HACCP certification and menu design.",
        "Node.js backend engineer with RESTful API experience"
    ],
    'job_title': [
        "Python Developer",
        "Executive Chef",
        "Python Developer",
        "Data Scientist",
        "Executive Chef",
        "Python Developer"
    ],
    'label': [1, 1, 0, 1, 1, 1]  # 1 = relevant, 0 = not relevant
}

df = pd.DataFrame(data)

df['input_text'] = df['resume_text'] + ' [SEP] ' + df['job_title']

vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')
X = vectorizer.fit_transform(df['input_text'])
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_clf = SVC(kernel='linear', probability=True)
svm_clf.fit(X_train, y_train)

y_pred = svm_clf.predict(X_test)
print(classification_report(y_test, y_pred))

# New resumes
new_resumes = [
    "Backend developer with Python, Flask, Django, and API experience.",
    "Experienced Executive Chef with kitchen staff leadership.",
    "Graphic designer with Photoshop and Figma knowledge."
]

job_title = "Python Developer"
new_input = [r + ' [SEP] ' + job_title for r in new_resumes]

# Vectorize and predict probabilities
X_new = vectorizer.transform(new_input)
probs = svm_clf.predict_proba(X_new)[:, 1]

# Rank
ranked = sorted(zip(new_resumes, probs), key=lambda x: x[1], reverse=True)

for i, (resume, score) in enumerate(ranked, 1):
    print(f"Rank {i}: Score={score:.2f} ‚Üí {resume}")

plt.figure(figsize=(8, 5))
sns.heatmap(metrics_df, annot=True, cmap="YlGnBu", fmt=".2f", cbar=True)
plt.title("Model Performance Heatmap (Logistic Regression vs SVM)")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Precomputed metrics
results = {
    "Logistic Regression": [0.83, 0.80, 0.75, 0.77],
    "SVM": [0.85, 0.82, 0.79, 0.80]
}

# Convert to DataFrame
metrics_df = pd.DataFrame(results, index=["Accuracy", "Precision", "Recall", "F1-score"])

# Heatmap
plt.figure(figsize=(8, 5))
sns.heatmap(metrics_df, annot=True, cmap="YlGnBu", fmt=".2f", linewidths=0.5)
plt.title("üîç Model Performance Heatmap")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""Save all models"""

import pickle

with open('/content/drive/MyDrive/Project/resume_screening/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)
print("tfidf_vectorizer.pkl saved")

with open('/content/drive/MyDrive/Project/resume_screening/logistic_model.pkl', 'wb') as f:
    pickle.dump(svm_clf, f)
print("logistic_model.pkl saved")

w2v_model.save("/content/drive/MyDrive/Project/resume_screening/word2vec.model")
print("word2vec.model saved")

"""Dashboard"""


# ngrok config add-authtoken
from pyngrok import ngrok
import time
import threading
import os

# Get the auth token from environment variables or hardcode it
# It's recommended to use environment variables for security
# os.environ["NGROK_AUTH_TOKEN"] = "YOUR_AUTH_TOKEN"
# You can get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken


thread = threading.Thread(target=run_streamlit)
thread.start()

# Wait a few seconds for Streamlit to start
time.sleep(5)

# Open a ngrok tunnel to the Streamlit port
# Make sure ngrok is installed and authenticated
try:
    public_url = ngrok.connect(8501)
    print(f"‚úÖ Streamlit app running at: {public_url}")
except Exception as e:
    print(f"Error starting ngrok tunnel: {e}")
    print("Please ensure ngrok is installed and you have set your authtoken.")
    print("You can install ngrok using `!pip install pyngrok` and set the authtoken using `!ngrok config add-authtoken YOUR_AUTH_TOKEN` in a new cell.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# 
# # Load models
# with open("tfidf_vectorizer.pkl", "rb") as f:
#     vectorizer = pickle.load(f)
# with open("logistic_model.pkl", "rb") as f:
#     model = pickle.load(f)
# 
# st.title("üîç AI Resume Screening System")
# resume = st.text_area("üìÑ Paste Resume", height=200)
# job = st.text_area("üßæ Paste Job Description", height=150)
# 
# if st.button("üéØ Match"):
#     if resume and job:
#         text = resume + " [SEP] " + job
#         vec = vectorizer.transform([text])
#         prob = model.predict_proba(vec)[0][1]
#         st.success(f"‚úÖ Match Score: {prob:.2f}")
#     else:
#         st.warning("Please fill both fields")
#

from pyngrok import conf

conf.get_default().auth_token = "2xunvMglR7LwdExboQx86uEViYv_7un7q9he4sCr44NxMptGr"

from pyngrok import ngrok

# Open tunnel
public_url = ngrok.connect(8501)
print("‚úÖ Public URL:", public_url)


from pyngrok import ngrok
import time
import threading


thread = threading.Thread(target=run)
thread.start()

time.sleep(5)
public_url = ngrok.connect(8501) # Pass 8501 as the address
print(f"‚úÖ Visit your app at: {public_url}")


import os

# Make sure the resume_folder variable is defined
# resume_folder = '/content/drive/MyDrive/Project/resume_screening/data' # Uncomment and modify if needed

resume_data_from_folder = []

# Iterate through files in the resume folder
for file_name in os.listdir(resume_folder):
    file_path = os.path.join(resume_folder, file_name)

    # Check if it's a file and not a directory
    if os.path.isfile(file_path):
        raw_text = extract_text_from_resume(file_path)
        resume_data_from_folder.append({'filename': file_name, 'raw_text': raw_text})



